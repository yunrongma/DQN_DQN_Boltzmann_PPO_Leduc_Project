import sys
import os
import random
import numpy as np
import tensorflow as tf
import logging
import rlcard

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))

from rlcard.utils import tournament
from rlcard.agents.random_agent import RandomAgent
from agents.dqn_boltzmann_agent import DQNAgentWithBoltzmann
from agents.dqn_agent import DQNAgentBase
from agents.ppo_agent import PPOAgentBase

results_dir = 'evaluation_results'
os.makedirs(results_dir, exist_ok=True)

# Set up logging
logging.basicConfig(
    filename=os.path.join(results_dir, 'exploitability_log.log'),
    filemode='w',
    format='%(asctime)s - %(message)s',
    level=logging.INFO
)

random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

# Initialize environment
env = rlcard.make('leduc-holdem', config={'allow_step_back': True})
action_num = env.game.get_num_actions()
state_shape = env.state_shape[0]

# Load trained agents
models_dir = 'models'

# Load Boltzmann agent
boltzmann_agent = DQNAgentWithBoltzmann(
    scope='dqn_boltzmann',
    action_num=action_num,
    state_shape=state_shape,
    mlp_layers=[32]
)
boltzmann_agent.load(os.path.join(models_dir, 'new4_boltzmann_T2_model.keras'))

# Load DQN agent
dqn_agent = DQNAgentBase(
    scope='dqn',
    action_num=action_num,
    state_shape=state_shape,
    mlp_layers=[32]
)
dqn_agent.load(os.path.join(models_dir, 'dqn_model4.keras'))

# Load PPO agent
ppo_agent = PPOAgentBase(
    scope='ppo',
    action_num=action_num,
    state_shape=state_shape,
    mlp_layers=[64, 64]
)
ppo_agent.load(
    policy_path=os.path.join(models_dir, 'ppo_leduc_model_policy.weights.h5'),
    value_path=os.path.join(models_dir, 'ppo_leduc_model_value.weights.h5')
)

# CFR agent for exploitability evaluation
from rlcard.agents.cfr_agent import CFRAgent
cfr_agent = CFRAgent(env)
cfr_training_iterations = 5000
print(f"Training CFRAgent for {cfr_training_iterations} iterations...")
for _ in range(cfr_training_iterations):
    cfr_agent.train()
print("CFRAgent training complete.")

# Set CFR agent as the opponent
env.set_agents([None, cfr_agent])

# Temperatures to evaluate for Boltzmann agent
temperatures = [0, 0.5, 1, 2, 5, 10]

# Evaluation settings
eval_episodes = 1000
results_file = os.path.join(results_dir, 'exploitability_results.txt')
os.makedirs(os.path.dirname(results_file), exist_ok=True)

# Evaluate agents
with open(results_file, 'w') as f:
    f.write("Agent,Temperature,Exploitability,Variance\n")
    logging.info("Evaluating exploitability for DQN, PPO, and Boltzmann agents...")

    # Evaluate DQN agent
    logging.info("Evaluating DQN agent...")
    print("Evaluating DQN agent...")
    env.set_agents([dqn_agent, cfr_agent])
    dqn_exploitabilities = []
    for _ in range(eval_episodes):
        _, payoffs = env.run(is_training=False)
        dqn_exploitabilities.append(-payoffs[0])

    avg_exploitability = np.mean(dqn_exploitabilities)
    variance_exploitability = np.var(dqn_exploitabilities)
    f.write(f"DQN,None,{avg_exploitability:.4f},{variance_exploitability:.4f}\n")
    logging.info(f"DQN agent: Exploitability = {avg_exploitability:.4f}, Variance = {variance_exploitability:.4f}")
    print(f"DQN agent: Exploitability = {avg_exploitability:.4f}, Variance = {variance_exploitability:.4f}")

    # Evaluate PPO agent
    logging.info("Evaluating PPO agent...")
    print("Evaluating PPO agent...")
    env.set_agents([ppo_agent, cfr_agent])
    ppo_exploitabilities = []
    for _ in range(eval_episodes):
        _, payoffs = env.run(is_training=False)
        ppo_exploitabilities.append(-payoffs[0])

    avg_exploitability = np.mean(ppo_exploitabilities)
    variance_exploitability = np.var(ppo_exploitabilities)
    f.write(f"PPO,None,{avg_exploitability:.4f},{variance_exploitability:.4f}\n")
    logging.info(f"PPO agent: Exploitability = {avg_exploitability:.4f}, Variance = {variance_exploitability:.4f}")
    print(f"PPO agent: Exploitability = {avg_exploitability:.4f}, Variance = {variance_exploitability:.4f}")

    # Evaluate Boltzmann agent at different temperatures
    for temp in temperatures:
        logging.info(f"Evaluating Boltzmann agent at temperature {temp}...")
        print(f"Evaluating Boltzmann agent at temperature {temp}...")
        boltzmann_agent.temperature_test_action(temp)
        env.set_agents([boltzmann_agent, cfr_agent])
        boltzmann_exploitabilities = []

        for _ in range(eval_episodes):
            _, payoffs = env.run(is_training=False)
            boltzmann_exploitabilities.append(-payoffs[0])

        avg_exploitability = np.mean(boltzmann_exploitabilities)
        variance_exploitability = np.var(boltzmann_exploitabilities)
        f.write(f"Boltzmann,{temp},{avg_exploitability:.4f},{variance_exploitability:.4f}\n")
        logging.info(f"Boltzmann agent (T={temp}): Exploitability = {avg_exploitability:.4f}, Variance = {variance_exploitability:.4f}")
        print(f"Boltzmann agent (T={temp}): Exploitability = {avg_exploitability:.4f}, Variance = {variance_exploitability:.4f}")

logging.info(f"Evaluation complete. Results saved to {results_file}.")
print(f"Evaluation complete. Results saved to {results_file}.")